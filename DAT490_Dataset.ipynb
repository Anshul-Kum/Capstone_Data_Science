{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and Storing Dataset  \n",
        "\n",
        "- We are planning to merge the reviews and metadata files from all the states.\n",
        "- Then concatenate those files and create a single parquet file.  \n",
        "\n",
        "The objective is to get a dataset that we can use through the entirety of our project."
      ],
      "metadata": {
        "id": "bhmAJg0eTe9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAWsy84AR9xj"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j2Rle19SLDM",
        "outputId": "a8ee5bfc-f59e-4040-d6ea-42463d5a735c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "project_id = \"sharp-matter-449521-u2\"\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ligE_YYjDXJl"
      },
      "outputs": [],
      "source": [
        "states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n",
        "    \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n",
        "    \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New_Hampshire\", \"New_Jersey\",\n",
        "    \"New_Mexico\", \"New_York\", \"North_Carolina\", \"North_Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode_Island\", \"South_Carolina\",\n",
        "    \"South_Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West_Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "kMYxG_HGi9kX",
        "outputId": "68a3e93e-3c53-41f9-ca46-28a9b1a80394",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Alabama-reviews.json.gz from https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-Alabama_10.json.gz...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7ed87ba88c10>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mreview_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-{state}_10.json.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mmetadata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/meta-{state}.json.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mupload_json_gz_to_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reviews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0mupload_json_gz_to_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-7ed87ba88c10>\u001b[0m in \u001b[0;36mupload_json_gz_to_gcs\u001b[0;34m(url, state_name, file_type)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{state_name}-{file_type}.json.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Download the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdownload_file_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Upload the file directly to GCS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-7ed87ba88c10>\u001b[0m in \u001b[0;36mdownload_file_fast\u001b[0;34m(url, file_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Read entire binary content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbinfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mbinfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mIncompleteRead\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdetect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \"\"\"\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "from urllib.request import urlopen\n",
        "# Initializing Google Cloud Storage Client\n",
        "client = storage.Client()\n",
        "\n",
        "bucket_name = \"my-group-project-bucket-dat490\"\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "def download_file_fast(url, file_path):\n",
        "    \"\"\"Download the file from URL and save it locally.\"\"\"\n",
        "    print(f\"Downloading {file_path} from {url}...\")\n",
        "    try:\n",
        "        with urlopen(url) as resp:\n",
        "            data = resp.read()  # Read entire binary content\n",
        "            with open(file_path, \"wb\") as binfile:\n",
        "                binfile.write(data)\n",
        "        print(f\"Download successful: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "def upload_json_gz_to_gcs(url, state_name, file_type):\n",
        "    \"\"\"Download and upload .json.gz file to GCS.\"\"\"\n",
        "    file_path = f\"{state_name}-{file_type}.json.gz\"\n",
        "    # Downloading the file\n",
        "    download_file_fast(url, file_path)\n",
        "    # Uploading the file directly to GCS\n",
        "    try:\n",
        "        blob = bucket.blob(file_path)\n",
        "        blob.upload_from_filename(file_path)\n",
        "        print(f\"{file_path} uploaded successfully to GCS.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file to GCS: {e}\")\n",
        "\n",
        "for state in states:\n",
        "  review_url = f\"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-{state}_10.json.gz\"\n",
        "  metadata_url = f\"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/meta-{state}.json.gz\"\n",
        "  upload_json_gz_to_gcs(review_url, state, \"reviews\")\n",
        "  upload_json_gz_to_gcs(metadata_url, state, \"metadata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ9crYM4t3v9"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Initializing GCS client\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(\"my-group-project-bucket-dat490\")\n",
        "destination_bucket = client.bucket(\"final_dataset_dat490\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfjJ52_Et6iC",
        "outputId": "3b0724da-9c28-4ac5-9d32-91f3e97c71a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 .json.gzip files in my-group-project-bucket-dat490\n"
          ]
        }
      ],
      "source": [
        "# Checking if the files have been downloaded in the bucket or not\n",
        "from google.cloud import storage\n",
        "\n",
        "bucket = client.bucket(\"my-group-project-bucket-dat490\")\n",
        "\n",
        "blobs = list(client.list_blobs(bucket))\n",
        "\n",
        "json_gzip_files = []\n",
        "\n",
        "for blob in blobs:\n",
        "    json_gzip_files.append(blob.name)\n",
        "\n",
        "print(f\"Found {len(json_gzip_files)} .json.gzip files in {\"my-group-project-bucket-dat490\"}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNZnzvxfaFRH",
        "outputId": "2248106d-6232-403f-e20c-464ae89f063a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [68.9 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,712 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,694 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,338 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,652 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,666 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,958 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Fetched 25.3 MB in 3s (8,361 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libice-dev libsm-dev libxkbfile1 libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre x11-utils\n",
            "Suggested packages:\n",
            "  libice-doc libsm-doc libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libice-dev libsm-dev libxkbfile1 libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk openjdk-11-jre\n",
            "  x11-utils\n",
            "0 upgraded, 14 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 5,522 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.26+4-1ubuntu1~22.04 [214 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.26+4-1ubuntu1~22.04 [1,341 kB]\n",
            "Fetched 5,522 kB in 2s (2,909 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../04-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../06-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../07-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../08-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libice-dev:amd64.\n",
            "Preparing to unpack .../09-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n",
            "Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Selecting previously unselected package libsm-dev:amd64.\n",
            "Preparing to unpack .../10-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n",
            "Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../12-openjdk-11-jre_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../13-openjdk-11-jdk_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.13)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2024.10.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.68.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Installing all the necessary libraries\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark\n",
        "!pip install gcsfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_kpJgqZaPAc",
        "outputId": "3fb58949-1e69-4c31-9679-28786ebe8cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-04 19:05:03--  https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.207, 74.125.142.207, 74.125.195.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40713341 (39M) [application/java-archive]\n",
            "Saving to: ‘/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar’\n",
            "\n",
            "gcs-connector-hadoo 100%[===================>]  38.83M   188MB/s    in 0.2s    \n",
            "\n",
            "2025-03-04 19:05:03 (188 MB/s) - ‘/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar’ saved [40713341/40713341]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading the Google Cloud Storage Connector\n",
        "# Without this, we cannot read/write in Google Cloud Storage\n",
        "!wget -P /usr/lib/spark/jars/ https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI7WHXkGYwpq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataProcessing\") \\\n",
        "    .config(\"spark.jars\", \"/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n",
        "    \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n",
        "    \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New_Hampshire\", \"New_Jersey\",\n",
        "    \"New_Mexico\", \"New_York\", \"North_Carolina\", \"North_Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode_Island\", \"South_Carolina\",\n",
        "    \"South_Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West_Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "western_states1 = [\n",
        "    \"Alaska\",\n",
        "    \"Arizona\",\n",
        "    \"California\",\n",
        "    \"Colorado\",\n",
        "    \"Hawaii\",\n",
        "    \"Idaho\"\n",
        "]\n",
        "\n",
        "western_states2 = [\n",
        "    \"Montana\",\n",
        "    \"Nevada\",\n",
        "    \"New_Mexico\",\n",
        "    \"Oregon\",\n",
        "    \"Utah\",\n",
        "    \"Washington\",\n",
        "    \"Wyoming\"\n",
        "]\n",
        "\n",
        "northeast_states1 = [\n",
        "    \"Connecticut\",\n",
        "    \"Maine\",\n",
        "    \"Massachusetts\",\n",
        "    \"New_Hampshire\",\n",
        "    \"Rhode_Island\",\n",
        "    \"Vermont\",\n",
        "    \"New_Jersey\",\n",
        "    \"New_York\",\n",
        "    \"Pennsylvania\"\n",
        "]\n",
        "\n",
        "midwest_states1 = [\n",
        "    \"Illinois\",\n",
        "    \"Indiana\",\n",
        "    \"Iowa\",\n",
        "    \"Kansas\",\n",
        "    \"Michigan\",\n",
        "    \"Minnesota\"\n",
        "]\n",
        "midwest_states2 = [\n",
        "     \"Missouri\",\n",
        "    \"Nebraska\",\n",
        "    \"North_Dakota\",\n",
        "    \"Ohio\",\n",
        "    \"South_Dakota\",\n",
        "    \"Wisconsin\"\n",
        "]\n",
        "\n",
        "south_states1 = [\n",
        "    \"Delaware\",\n",
        "    \"Florida\",\n",
        "    \"Georgia\",\n",
        "    \"Maryland\",\n",
        "    \"North_Carolina\",\n",
        "    \"South_Carolina\",\n",
        "    \"Virginia\",\n",
        "    \"West_Virginia\"\n",
        "]\n",
        "south_states2 = [\n",
        "    \"Alabama\",\n",
        "    \"Kentucky\",\n",
        "    \"Mississippi\",\n",
        "    \"Tennessee\",\n",
        "    \"Arkansas\",\n",
        "    \"Louisiana\",\n",
        "    \"Oklahoma\",\n",
        "    \"Texas\"\n",
        "]"
      ],
      "metadata": {
        "id": "wm_fcrfP7-ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "import subprocess\n",
        "\n",
        "# Define GCP Bucket Paths\n",
        "input_bucket = \"my-group-project-bucket-dat490\"\n",
        "output_bucket = \"final_dataset_dat490\"\n",
        "bucket_path = \"gs://my-group-project-bucket-dat490/\"\n",
        "temp_output_path = \"gs://final_dataset_dat490/temp_parquet_output\"\n",
        "\n",
        "# Lists of states to process\n",
        "state_lists = {\n",
        "    \"western_states1\": western_states1,\n",
        "    \"western_states2\": western_states2,\n",
        "    \"northeast_states1\": northeast_states1,\n",
        "    \"midwest_states1\": midwest_states1,\n",
        "    \"midwest_states2\": midwest_states2,\n",
        "    \"south_states1\": south_states1,\n",
        "    \"south_states2\": south_states2,\n",
        "}\n",
        "\n",
        "for list_name, state_list in state_lists.items():\n",
        "    output_path = f\"gs://{output_bucket}/final_data_{list_name}.parquet\"\n",
        "    final_df = None\n",
        "\n",
        "    for state in state_list:\n",
        "        reviews_path = f\"gs://{input_bucket}/{state}-reviews.json.gz\"\n",
        "        metadata_path = f\"gs://{input_bucket}/{state}-metadata.json.gz\"\n",
        "\n",
        "        reviews_df = spark.read.json(reviews_path)\n",
        "        metadata_df = spark.read.json(metadata_path)\n",
        "\n",
        "        # Removing MISC column as it contains random description of our dataset\n",
        "        if \"MISC\" in metadata_df.columns:\n",
        "            metadata_df = metadata_df.drop(\"MISC\")\n",
        "\n",
        "        reviews_sampled = reviews_df.dropDuplicates([\"gmap_id\"])\n",
        "        metadata_sampled = metadata_df.dropDuplicates([\"gmap_id\"])\n",
        "\n",
        "        reviews_sampled = reviews_sampled.withColumnRenamed(\"name\", \"customer_name\")\n",
        "        metadata_sampled = metadata_sampled.withColumnRenamed(\"name\", \"business_name\")\n",
        "\n",
        "        # Joining reviews and metadata on \"gmap_id\"\n",
        "        merged_df = reviews_sampled.join(metadata_sampled, on=\"gmap_id\", how=\"inner\")\n",
        "\n",
        "        merged_df = merged_df.withColumn(\"state\", lit(state))\n",
        "\n",
        "        if final_df is None:\n",
        "            final_df = merged_df\n",
        "        else:\n",
        "            final_df = final_df.union(merged_df)\n",
        "\n",
        "        print(f\"Processed: {state} in {list_name}\")\n",
        "\n",
        "    final_df = final_df.coalesce(1)\n",
        "\n",
        "    final_df.write.mode(\"overwrite\").parquet(temp_output_path)\n",
        "\n",
        "    try:\n",
        "        list_files = subprocess.check_output(f\"gsutil ls {temp_output_path}/\", shell=True).decode(\"utf-8\").split(\"\\n\")\n",
        "        parquet_files = [f for f in list_files if f.endswith(\".parquet\")]\n",
        "\n",
        "        if parquet_files:\n",
        "            parquet_file = parquet_files[0]\n",
        "            subprocess.run(f\"gsutil mv {parquet_file} {output_path}\", shell=True)\n",
        "            subprocess.run(f\"gsutil rm -r {temp_output_path}\", shell=True)\n",
        "            print(f\"Final Parquet file saved at {output_path}\")\n",
        "        else:\n",
        "            print(\"No Parquet file found in temporary directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error renaming Parquet file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy3MXFJ8v_sK",
        "outputId": "32987cee-e55c-4130-df6f-4ec0986621a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed: Alabama\n",
            "✅ Processed: Kentucky\n",
            "✅ Processed: Mississippi\n",
            "✅ Processed: Tennessee\n",
            "✅ Processed: Arkansas\n",
            "✅ Processed: Louisiana\n",
            "✅ Processed: Oklahoma\n",
            "✅ Processed: Texas\n",
            "✅ Final Parquet file saved at gs://final_dataset_dat490/final_data_south2.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"final_dataset_dat490\"\n",
        "parquet_files = [\n",
        "    f\"gs://{bucket_name}/final_data_midwest1.parquet\",\n",
        "    f\"gs://{bucket_name}/final_data_midwest2.parquet\",\n",
        "    f\"gs://{bucket_name}/final_data_northeast1.parquet\",\n",
        "    f\"gs://{bucket_name}/final_data_south1.parquet\",\n",
        "    f\"gs://{bucket_name}/final_data_south2.parquet\",\n",
        "    f\"gs://{bucket_name}/final_data_west1.parquet\",\n",
        "    f\"gs://{bucket_name}/final_data_west2.parquet\"\n",
        "]"
      ],
      "metadata": {
        "id": "3Wbd9q47PaOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = spark.read.parquet(*parquet_files)"
      ],
      "metadata": {
        "id": "LbLnwOZKPcBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dU1LukPPi4K",
        "outputId": "b2b7f7be-2389-4a4f-ba7b-f3c7ee7f5f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2988811"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.repartition(1).write.mode(\"overwrite\").parquet(\"gs://final_dataset_dat490/dat490_final_dataset.parquet\")"
      ],
      "metadata": {
        "id": "5jcWte7ujbqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"gs://final_dataset_dat490/dat490_final_dataset.parquet\")\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0rksm70Pq0o",
        "outputId": "2ce6a422-51b1-43dc-fb19-1ed44d461766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2988811"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "8WEI0cxzlQBg",
        "outputId": "12d543eb-bf6a-4257-d15f-6e9ae9ca8a93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- gmap_id: string (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- pics: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- url: array (nullable = true)\n",
            " |    |    |    |-- element: string (containsNull = true)\n",
            " |-- rating: long (nullable = true)\n",
            " |-- resp: struct (nullable = true)\n",
            " |    |-- text: string (nullable = true)\n",
            " |    |-- time: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- time: long (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- avg_rating: double (nullable = true)\n",
            " |-- category: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- hours: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- business_name: string (nullable = true)\n",
            " |-- num_of_reviews: long (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- relative_results: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully created the final dataset."
      ],
      "metadata": {
        "id": "2wkhE2iZUeyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/"
      ],
      "metadata": {
        "id": "pHbrGzskVLa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "[1] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian Mcauley\n",
        "The 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2023\n",
        "\n",
        "[2] Jiacheng Li, Jingbo Shang, Julian McAuley\n",
        "Annual Meeting of the Association for Computational Linguistics (ACL), 2022  "
      ],
      "metadata": {
        "id": "WHLJvSMLUmyr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}